<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>ISLR 5: Resampling Methods • StatisticalLearning</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">StatisticalLearning</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/03-LinearRegression.html">ISLR 3: Linear Regression</a>
    </li>
    <li>
      <a href="../articles/04-Classification.html">ISLR 4: Classification</a>
    </li>
    <li>
      <a href="../articles/05-CV-Resampling-Methods.html">ISLR 5: Resampling Methods</a>
    </li>
    <li>
      <a href="../articles/06-Model-Selection.html">ISLR 6: Linear Model Selection and Regularization</a>
    </li>
    <li>
      <a href="../articles/07-Nonlinear.html">Moving Beyond Linearity</a>
    </li>
    <li>
      <a href="../articles/08-Trees.html">Tree-Based Methods</a>
    </li>
    <li>
      <a href="../articles/09-SVM.html">Support Vector Machines</a>
    </li>
    <li>
      <a href="../articles/10-Unsupervised.html">Unsupervised Learning</a>
    </li>
    <li>
      <a href="../articles/quiz-05-CV-Resampling-Methods.html">ISLR: Bootstrap quiz</a>
    </li>
    <li>
      <a href="../articles/quiz-07-Nonlinear-Functions-in-R.html">ISLR: Nonlinear functions quiz</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>ISLR 5: Resampling Methods</h1>
            
          </div>

    
    
<div class="contents">

<div id="introduction" class="section level2">
<h2 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h2>
<p>We are going to use the <code>Auto</code> data from the <code>ISLR</code> package to illustrate various resampling methods.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ISLR)
<span class="kw">data</span>(Auto)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">?Auto</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(Auto)</code></pre></div>
<pre><code>## [1] 392   9</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">names</span>(Auto)</code></pre></div>
<pre><code>## [1] "mpg"          "cylinders"    "displacement" "horsepower"  
## [5] "weight"       "acceleration" "year"         "origin"      
## [9] "name"</code></pre>
<p>A plot is always a nice place to start with a new data set.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(mpg <span class="op">~</span><span class="st"> </span>horsepower, <span class="dt">data =</span> Auto)</code></pre></div>
<p><img src="05-CV-Resampling-Methods_files/figure-html/unnamed-chunk-4-1.png" width="672"></p>
</div>
<div id="the-leave-one-out-cross-validation-lcoov-method-" class="section level2">
<h2 class="hasAnchor">
<a href="#the-leave-one-out-cross-validation-lcoov-method-" class="anchor"></a>The Leave-One-Out Cross-Validation (LCOOV) method.</h2>
<p>First, lets run a <code>glm</code> model on the <code>Auto</code> data set.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">glm_auto &lt;-<span class="st"> </span><span class="kw">glm</span>(mpg <span class="op">~</span><span class="st"> </span>horsepower, <span class="dt">data =</span> Auto)</code></pre></div>
<p>Next, load the <code>boot</code> package and check out the documentation for the Cross-validation for Generalized Linear Models <code>cv.glm</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(boot)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><a href="http://www.rdocumentation.org/packages/boot/topics/cv.glm">?cv.glm</a></code></pre></div>
<p>Then, execute the <code>cv.glm</code> function using the <code>Auto</code> data set and the <code>glm_auto</code> model above. return the delta parameter. Notice that we do not define the <code>K</code> argument in the <code>cv.glm</code> function, which number of groups into which the data should be split to estimate the cross-validation prediction error. The default for <code>K</code> is to set it equal to the number of observations in the data, which gives the Leave-One-Out Cross-Validation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">glm_auto_lcoov &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/boot/topics/cv.glm">cv.glm</a></span>(Auto, glm_auto)
glm_auto_lcoov<span class="op">$</span>delta</code></pre></div>
<pre><code>## [1] 24.23151 24.23114</code></pre>
<p>You may notice, your R session hangs for a bit as the function runs. This is because <code>cv.glm</code> uses brute force to repeatedly fit the model on <code>n</code> observation, removing 1 observation each time. Because this is a linear model, we can dramatically speed up the computation with the formula displayed in section 5.2 of ISLR (pg. 180), which uses the results of the computation stored in the <code>glm_auto</code> model object. Basically, we take the <code>mean</code> of the squared residuals of <code>glm_auto</code> and divide them by the squared vector containing the diagonal of the <code>hat</code> matrix.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">loocv &lt;-<span class="st"> </span><span class="cf">function</span>(x){
        
          hat &lt;-<span class="st"> </span><span class="kw">lm.influence</span>(x)<span class="op">$</span>hat
        
        <span class="kw">mean</span>((<span class="kw">residuals</span>(x)<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>hat))<span class="op">^</span><span class="dv">2</span>)
}</code></pre></div>
<p>Is the new function faster? We can use the <code>microbenchmark</code> package to time each function in milliseconds, and we see that the new <code>loocv</code> function is much faster.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(microbenchmark)
<span class="kw">microbenchmark</span>(<span class="kw"><a href="http://www.rdocumentation.org/packages/boot/topics/cv.glm">cv.glm</a></span>(Auto, glm_auto)<span class="op">$</span>delta, <span class="kw">loocv</span>(glm_auto), <span class="dt">times =</span> <span class="dv">10</span>, <span class="dt">unit =</span> <span class="st">"ms"</span>)</code></pre></div>
<pre><code>## Unit: milliseconds
##                          expr         min         lq        mean
##  cv.glm(Auto, glm_auto)$delta 1188.119815 1205.69117 1229.761462
##               loocv(glm_auto)    0.817638    0.88777    1.511502
##        median          uq         max neval cld
##  1233.4534250 1253.614031 1258.758996    10   b
##     0.9377185    0.976377    6.007078    10  a</code></pre>

<p>Next, lets use a <code>for</code> loop with our <code>loocv</code> function to efficiently create 5 new polynomial versions of the previous model, regressing <code>horsepower</code> against <code>mpg</code> and see if the results improve as polynomial order increases.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cv.error &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">5</span>)
degree &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">5</span>

<span class="cf">for</span>(d <span class="cf">in</span> degree){
        
  glm_fit &lt;-<span class="st"> </span><span class="kw">glm</span>(mpg <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(horsepower, d), <span class="dt">data =</span> Auto)
  
  cv.error[d] &lt;-<span class="st"> </span><span class="kw">loocv</span>(glm_fit)
}

<span class="kw">plot</span>(degree, cv.error, <span class="dt">type =</span> <span class="st">"b"</span>, <span class="dt">col =</span> <span class="st">"blue"</span>, <span class="dt">pch =</span> <span class="dv">16</span>,
     <span class="dt">main =</span> <span class="st">"Leave-One-Out Cross-Validation"</span>, <span class="dt">xlab =</span> <span class="st">"Degree of Polynomial"</span>)</code></pre></div>
<p><img src="05-CV-Resampling-Methods_files/figure-html/unnamed-chunk-11-1.png" width="672"></p>

</div>
<div id="the-k-fold-cross-validation" class="section level2">
<h2 class="hasAnchor">
<a href="#the-k-fold-cross-validation" class="anchor"></a>The K-fold Cross-Validation</h2>
<p>This time, we can use the same model, but return to the <code>cv.glm</code> function so that we can define <code>K</code> as equal to 10. First, initialize the <code>cv.error10</code> and <code>degree</code> vectors, run the <code>for</code> loop. Typically, K-fold outperforms LOOCV, but in this example, they are very similar as displayed in <code>plot</code> of the results. As in the previous graph, blue reflects the LOOCV errors, while red reflects K-Fold = 10.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cv.error10 &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">5</span>)
degree &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">5</span>

<span class="cf">for</span>(d <span class="cf">in</span> degree){
  glm_fit &lt;-<span class="st"> </span><span class="kw">glm</span>(mpg <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(horsepower, d), <span class="dt">data =</span> Auto)
  cv.error10[d] &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/boot/topics/cv.glm">cv.glm</a></span>(Auto, glm_fit, <span class="dt">K =</span> <span class="dv">10</span>)<span class="op">$</span>delta[<span class="dv">1</span>]
}

<span class="kw">plot</span>(degree, cv.error, <span class="dt">type =</span> <span class="st">"b"</span>, <span class="dt">col =</span> <span class="st">"blue"</span>, <span class="dt">pch =</span> <span class="dv">16</span>, 
     <span class="dt">main =</span> <span class="st">"Comparing LOO &amp; K-Fold Cross-Validation"</span>, <span class="dt">xlab =</span> <span class="st">"Degree of Polynomial"</span>)
<span class="kw">lines</span>(degree, cv.error10, <span class="dt">type =</span> <span class="st">"b"</span>, <span class="dt">col =</span> <span class="st">"red"</span>, <span class="dt">pch =</span> <span class="dv">16</span>)</code></pre></div>
<p><img src="05-CV-Resampling-Methods_files/figure-html/unnamed-chunk-12-1.png" width="672"></p>
</div>
<div id="bootstrap" class="section level2">
<h2 class="hasAnchor">
<a href="#bootstrap" class="anchor"></a>Bootstrap</h2>
<p>Suppose that we wish to invest a fixed. sum of money in two financial assets that yield returns of X and Y, where X and Y are random quantities. We will invest a fraction of our money in X, and will invest the remaining <span class="math inline">\(1 - \alpha\)</span> in Y. We wish to choose <span class="math inline">\(\alpha\)</span> to minimize the total risk, or variance, of our investment. In other words, we want to minimize <span class="math inline">\(Var(\alpha X + (1-\alpha)Y)\)</span>. One can show that the value that minimizes the risk is given by</p>
<p><span class="math display">\[\alpha = \frac{\sigma^2_Y - \sigma_{XY}}{\sigma^2_X + \sigma^2_Y - 2\sigma_{XY}}\]</span></p>
<p>where <span class="math inline">\(\sigma^2_X = Var(X)\)</span>, <span class="math inline">\(\sigma^2_Y = Var(Y)\)</span>, and <span class="math inline">\(\sigma_{XY} = Cov(X,Y)\)</span>.</p>
<p>However, the values of <span class="math inline">\(\sigma^2_X\)</span>, <span class="math inline">\(\sigma^2_Y\)</span>, and <span class="math inline">\(\sigma_{XY}\)</span> are unknown. We can compute estimates for these quantities, <span class="math inline">\(\hat\sigma^2_X\)</span>, <span class="math inline">\(\hat\sigma^2_Y\)</span>, and <span class="math inline">\(\hat\sigma_{XY}\)</span>, using a data set that contains measurements for <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>We can then estimate the value of <span class="math inline">\(\alpha\)</span> that minimizes the variance of our investment using:</p>
<p><span class="math display">\[\hat\alpha = \frac{\hat\sigma^2_Y - \hat\sigma_{XY}}{\hat\sigma^2_X + \hat\sigma^2_Y - 2\hat\sigma_{XY}}\]</span></p>
<p>Load the <code>Portfolio</code> data set from the <code>ISLR</code> package, containing 100 returns for two assets, X and Y.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(<span class="st">"Portfolio"</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(Y <span class="op">~</span><span class="st"> </span>X, <span class="dt">data =</span> Portfolio, <span class="dt">col =</span> <span class="st">"darkgreen"</span>, <span class="dt">type =</span> <span class="st">"p"</span>, <span class="dt">pch =</span> <span class="dv">16</span>)</code></pre></div>
<p><img src="05-CV-Resampling-Methods_files/figure-html/unnamed-chunk-14-1.png" width="672"></p>
<p>Create the <span class="math inline">\(\alpha\)</span> function defined above and compute alpha.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">alpha &lt;-<span class="st"> </span><span class="cf">function</span>(x, y) {
        
          var_x &lt;-<span class="st"> </span><span class="kw">var</span>(x)
          var_y &lt;-<span class="st"> </span><span class="kw">var</span>(y)
          cov_xy &lt;-<span class="st"> </span><span class="kw">cov</span>(x, y)
          
(var_y <span class="op">-</span><span class="st"> </span>cov_xy)<span class="op">/</span>(var_x <span class="op">+</span><span class="st"> </span>var_y <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span>cov_xy)
          
}

<span class="kw">alpha</span>(Portfolio<span class="op">$</span>X, Portfolio<span class="op">$</span>Y)</code></pre></div>
<pre><code>## [1] 0.5758321</code></pre>
<p>Great! But what is the standard error of <span class="math inline">\(\alpha\)</span>? What is it’s variability? This is a case for bootstrap resampling and we can use the <code>boot</code> function to help us.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(boot)
<span class="kw">args</span>(boot)</code></pre></div>
<pre><code>## function (data, statistic, R, sim = "ordinary", stype = c("i", 
##     "f", "w"), strata = rep(1, n), L = NULL, m = 0, weights = NULL, 
##     ran.gen = function(d, p) d, mle = NULL, simple = FALSE, ..., 
##     parallel = c("no", "multicore", "snow"), ncpus = getOption("boot.ncpus", 
##         1L), cl = NULL) 
## NULL</code></pre>
<p>Notice in the boot function, the second argument <code>statistic</code> is where we will place the alpha function. When consulting the documentation in <code><a href="http://www.rdocumentation.org/packages/boot/topics/boot">?boot</a></code> we find that statistic takes at least two arguments. The first, being the original data and the second needs to be a vector of indices defining the bootstrap sample. The <code>boot</code> package will generate the bootstrap sample from the original data and whatever function defines the <code>statistic</code> argument must be able to use it. For our example, we can make a wrapper function preparing the <code>alpha</code> function for bootstrap.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">alpha_boot &lt;-<span class="st"> </span><span class="cf">function</span>(data, index){
        
                  args &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">x =</span> data[index, ]<span class="op">$</span>X, 
                               <span class="dt">y =</span> data[index, ]<span class="op">$</span>Y)
                   <span class="kw">do.call</span>(<span class="st">"alpha"</span>, args)
}

<span class="kw">alpha_boot</span>(Portfolio, <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>)</code></pre></div>
<pre><code>## [1] 0.5758321</code></pre>
<p>The <code>alpha_boot</code> function returns the same value as the <code>alpha</code> function, so we know it works. Next, lets pass a random sample to the <code>index</code> argument. We expect to get different results from running the <code>alpha</code> function on the original data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)

<span class="kw">alpha_boot</span>(<span class="dt">data =</span> Portfolio, <span class="dt">index =</span> <span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">100</span>, <span class="dv">100</span>, <span class="dt">replace =</span> <span class="ot">TRUE</span>))</code></pre></div>
<pre><code>## [1] 0.5963833</code></pre>
<p>The bootstrap does the above calculation, but it does it as many times as we like, depending on what we define for <code>R</code>. Printing the results gives us an estimate of the standard error of the original <span class="math inline">\(\alpha\)</span> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">boot.out &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/boot/topics/boot">boot</a></span>(Portfolio, alpha_boot, <span class="dt">R =</span> <span class="dv">10000</span>)

boot.out</code></pre></div>
<pre><code>## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = Portfolio, statistic = alpha_boot, R = 10000)
## 
## 
## Bootstrap Statistics :
##      original     bias    std. error
## t1* 0.5758321 0.00180545  0.09079434</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(boot.out)</code></pre></div>
<p><img src="05-CV-Resampling-Methods_files/figure-html/unnamed-chunk-19-1.png" width="672"></p>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#introduction">Introduction</a></li>
      <li><a href="#the-leave-one-out-cross-validation-lcoov-method-">The Leave-One-Out Cross-Validation (LCOOV) method.</a></li>
      <li><a href="#the-k-fold-cross-validation">The K-fold Cross-Validation</a></li>
      <li><a href="#bootstrap">Bootstrap</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Justin M. Shea.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
